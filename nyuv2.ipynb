{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PG5Y-lIBp1g"
      },
      "source": [
        "# AI Intergration Project - Monodepth2\n",
        "In this project, we try to reimplement a Depth Estimation model from paper [Digging Into Self-Supervised Monocular Depth Estimation](https://arxiv.org/pdf/1806.01260.pdf) . In Supplementary Material, the authors show us many versions and varients of Monodepth2. From the result table, we decided to reimplement non-pretrained Resnet18-Encoder Monodepth2 with Auto-masking, min-reprojection and full-res multi-scale on Kitti and NYUv2 dataset (Self-supervised mono supervision). We didn't choose Resnet50 (better results) for our model because of expense of longer training and test times .\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Group's members: \\\\\n",
        "Nguyễn Đăng Hoài Nam : \\\\\n",
        "- student's ID: 2152181 \\\\\n",
        "- Mail: nam.nguyencshcmut@hcmut.edu.vn \\\\\n",
        "\n",
        "Lê Trần Nguyên Khoa : \\\\\n",
        "- student's ID: 2152674 \\\\\n",
        "- Mail: khoa.lesteve@hcmut.edu.vn \\\\\n",
        "\n",
        "Đinh Việt Thành : \\\\\n",
        "- student's ID: 2152966 \\\\\n",
        "- Mail: thanh.dinhalex19cs@hcmut.edu.vn \\\\"
      ],
      "metadata": {
        "id": "uvk9_olptO4E"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0BdK-MKMbvn"
      },
      "source": [
        "# Mount Drive\n",
        "Remember to upload our folder to your Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "_83w0veZMhOd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f77408fd-654a-4a1d-c180-97b55e28487a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shus5Z4t6MjM"
      },
      "source": [
        "# Import Framework & Libs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "u0w060HGwrwM"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "from torch.utils.data import DataLoader\n",
        "from collections import OrderedDict\n",
        "from torchvision import transforms\n",
        "from collections import Counter\n",
        "from PIL import Image\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import torchvision.models as models\n",
        "import torch.utils.data as data\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "import PIL.Image as pil\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import skimage\n",
        "import random\n",
        "import torch\n",
        "import copy\n",
        "import time\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPlmcVa36wGz"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "M9ZJr4xl7SQ-"
      },
      "outputs": [],
      "source": [
        "def readlines(filename):\n",
        "    \"\"\"Read all the lines in a text file and return as a list\"\"\"\n",
        "    with open(filename, 'r') as f:\n",
        "        lines = f.read().splitlines()\n",
        "    return lines\n",
        "\n",
        "def pil_loader(path):\n",
        "    \"\"\"Open image\"\"\"\n",
        "    with open(path, 'rb') as f:\n",
        "        with Image.open(f) as img:\n",
        "            return img.convert('RGB')\n",
        "\n",
        "def disp_to_depth(disp, min_depth, max_depth):\n",
        "    \"\"\"Convert network's sigmoid output into depth prediction\n",
        "    The formula for this conversion is given in the 'additional considerations'\n",
        "    section of the paper.\n",
        "    \"\"\"\n",
        "    min_disp = 1 / max_depth\n",
        "    max_disp = 1 / min_depth\n",
        "    scaled_disp = min_disp + (max_disp - min_disp) * disp\n",
        "    depth = 1 / scaled_disp\n",
        "    return scaled_disp, depth\n",
        "\n",
        "def transformation_from_parameters(axisangle, translation, invert=False):\n",
        "    \"\"\"Convert the network's (axisangle, translation) output into a 4x4 matrix\n",
        "    \"\"\"\n",
        "    R = rot_from_axisangle(axisangle)\n",
        "    t = translation.clone()\n",
        "\n",
        "    if invert:\n",
        "        R = R.transpose(1, 2)\n",
        "        t *= -1\n",
        "\n",
        "    T = get_translation_matrix(t)\n",
        "\n",
        "    if invert:\n",
        "        M = torch.matmul(R, T)\n",
        "    else:\n",
        "        M = torch.matmul(T, R)\n",
        "\n",
        "    return M\n",
        "\n",
        "def get_translation_matrix(translation_vector):\n",
        "    \"\"\"Convert a translation vector into a 4x4 transformation matrix\n",
        "    \"\"\"\n",
        "    T = torch.zeros(translation_vector.shape[0], 4, 4).to(device=translation_vector.device)\n",
        "\n",
        "    t = translation_vector.contiguous().view(-1, 3, 1)\n",
        "\n",
        "    T[:, 0, 0] = 1\n",
        "    T[:, 1, 1] = 1\n",
        "    T[:, 2, 2] = 1\n",
        "    T[:, 3, 3] = 1\n",
        "    T[:, :3, 3, None] = t\n",
        "\n",
        "    return T\n",
        "\n",
        "def rot_from_axisangle(vec):\n",
        "    \"\"\"Convert an axisangle rotation into a 4x4 transformation matrix\n",
        "    Input 'vec' has to be Bx1x3\n",
        "    \"\"\"\n",
        "    angle = torch.norm(vec, 2, 2, True)\n",
        "    axis = vec / (angle + 1e-7)\n",
        "\n",
        "    ca = torch.cos(angle)\n",
        "    sa = torch.sin(angle)\n",
        "    C = 1 - ca\n",
        "\n",
        "    x = axis[..., 0].unsqueeze(1)\n",
        "    y = axis[..., 1].unsqueeze(1)\n",
        "    z = axis[..., 2].unsqueeze(1)\n",
        "\n",
        "    xs = x * sa\n",
        "    ys = y * sa\n",
        "    zs = z * sa\n",
        "    xC = x * C\n",
        "    yC = y * C\n",
        "    zC = z * C\n",
        "    xyC = x * yC\n",
        "    yzC = y * zC\n",
        "    zxC = z * xC\n",
        "\n",
        "    rot = torch.zeros((vec.shape[0], 4, 4)).to(device=vec.device)\n",
        "\n",
        "    rot[:, 0, 0] = torch.squeeze(x * xC + ca)\n",
        "    rot[:, 0, 1] = torch.squeeze(xyC - zs)\n",
        "    rot[:, 0, 2] = torch.squeeze(zxC + ys)\n",
        "    rot[:, 1, 0] = torch.squeeze(xyC + zs)\n",
        "    rot[:, 1, 1] = torch.squeeze(y * yC + ca)\n",
        "    rot[:, 1, 2] = torch.squeeze(yzC - xs)\n",
        "    rot[:, 2, 0] = torch.squeeze(zxC - ys)\n",
        "    rot[:, 2, 1] = torch.squeeze(yzC + xs)\n",
        "    rot[:, 2, 2] = torch.squeeze(z * zC + ca)\n",
        "    rot[:, 3, 3] = 1\n",
        "\n",
        "    return rot\n",
        "\n",
        "def upsample(x):\n",
        "    \"\"\"Upsample input tensor by a factor of 2\n",
        "    \"\"\"\n",
        "    return F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
        "\n",
        "def get_smooth_loss(disp, img):\n",
        "    \"\"\"Computes the smoothness loss for a disparity image\n",
        "    The color image is used for edge-aware smoothness\n",
        "    \"\"\"\n",
        "    grad_disp_x = torch.abs(disp[:, :, :, :-1] - disp[:, :, :, 1:])\n",
        "    grad_disp_y = torch.abs(disp[:, :, :-1, :] - disp[:, :, 1:, :])\n",
        "\n",
        "    grad_img_x = torch.mean(torch.abs(img[:, :, :, :-1] - img[:, :, :, 1:]), 1, keepdim=True)\n",
        "    grad_img_y = torch.mean(torch.abs(img[:, :, :-1, :] - img[:, :, 1:, :]), 1, keepdim=True)\n",
        "\n",
        "    grad_disp_x *= torch.exp(-grad_img_x)\n",
        "    grad_disp_y *= torch.exp(-grad_img_y)\n",
        "\n",
        "    return grad_disp_x.mean() + grad_disp_y.mean()\n",
        "\n",
        "def compute_depth_errors(gt, pred):\n",
        "    \"\"\"Computation of error metrics between predicted and ground truth depths\n",
        "    \"\"\"\n",
        "    thresh = torch.max((gt / pred), (pred / gt))\n",
        "    a1 = (thresh < 1.25     ).float().mean()\n",
        "    a2 = (thresh < 1.25 ** 2).float().mean()\n",
        "    a3 = (thresh < 1.25 ** 3).float().mean()\n",
        "\n",
        "    rmse = (gt - pred) ** 2\n",
        "    rmse = torch.sqrt(rmse.mean())\n",
        "\n",
        "    rmse_log = (torch.log(gt) - torch.log(pred)) ** 2\n",
        "    rmse_log = torch.sqrt(rmse_log.mean())\n",
        "\n",
        "    abs_rel = torch.mean(torch.abs(gt - pred) / gt)\n",
        "\n",
        "    sq_rel = torch.mean((gt - pred) ** 2 / gt)\n",
        "\n",
        "    return abs_rel, sq_rel, rmse, rmse_log, a1, a2, a3\n",
        "\n",
        "\n",
        "def sub2ind(matrixSize, rowSub, colSub):\n",
        "    \"\"\"Convert row, col matrix subscripts to linear indices\n",
        "    \"\"\"\n",
        "    m, n = matrixSize\n",
        "    return rowSub * (n-1) + colSub - 1\n",
        "\n",
        "\n",
        "def show_images(outputs):\n",
        "    disp = outputs[(\"disp\", 0)]\n",
        "    disp_resized = torch.nn.functional.interpolate(disp,\n",
        "    (512, 1024), mode=\"bilinear\", align_corners=False)\n",
        "\n",
        "    # Saving colormapped depth image\n",
        "    disp_resized_np = disp_resized.squeeze().detach().cpu().numpy()\n",
        "    vmax = np.percentile(disp_resized_np, 95)\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(disp_resized_np, cmap='magma', vmax=vmax)\n",
        "    plt.title(\"Disparity prediction\", fontsize=22)\n",
        "    plt.axis('off');\n",
        "    plt.show()\n",
        "\n",
        "def show_rgb(inputs):\n",
        "    inputs = inputs.cpu().squeeze().permute(1, 2, 0).numpy()\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(inputs)\n",
        "    plt.title(\"Input\", fontsize=22)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "def print_img(dir1, dir2):\n",
        "    model_dir = dir1\n",
        "    model = Monodepth()\n",
        "    model.load_checkpoint(model_dir)\n",
        "\n",
        "    image_path = dir2\n",
        "    input_image = pil.open(image_path).convert('RGB')\n",
        "    original_width, original_height = input_image.size\n",
        "\n",
        "    input_image_resized = input_image.resize((model.feed_width, model.feed_height), pil.LANCZOS)\n",
        "    input_image_pytorch = transforms.ToTensor()(input_image_resized).unsqueeze(0)\n",
        "\n",
        "    outputs = model.forward(input_image_pytorch)\n",
        "    disp = outputs[(\"disp\", 0)]\n",
        "\n",
        "    disp_resized = torch.nn.functional.interpolate(disp,\n",
        "        (original_height, original_width), mode=\"bilinear\", align_corners=False)\n",
        "\n",
        "    # Saving colormapped depth image\n",
        "    disp_resized_np = disp_resized.squeeze().cpu().numpy()\n",
        "    vmax = np.percentile(disp_resized_np, 95)\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.subplot(211)\n",
        "    plt.imshow(input_image)\n",
        "    plt.title(\"Input\", fontsize=22)\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(212)\n",
        "    plt.imshow(disp_resized_np, cmap='magma', vmax=vmax)\n",
        "    plt.title(\"Disparity prediction\", fontsize=22)\n",
        "    plt.axis('off');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77r0aWEH8xPh"
      },
      "source": [
        "# Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nyc0evYv84xw"
      },
      "source": [
        "Monodepth dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "igzrEaYi80_4"
      },
      "outputs": [],
      "source": [
        "class MonoDataset(data.Dataset):\n",
        "    def __init__(self, data_path, filenames, height, width, frame_idxs, num_scales, is_train=False, img_ext='.jpg'):\n",
        "        super(MonoDataset, self).__init__()\n",
        "\n",
        "        self.data_path = data_path\n",
        "        self.filenames = filenames\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.num_scales = num_scales\n",
        "        self.interp = Image.Resampling.LANCZOS\n",
        "        self.frame_idxs = frame_idxs\n",
        "        self.is_train = is_train\n",
        "        self.img_ext = img_ext\n",
        "        self.loader = pil_loader\n",
        "        self.to_tensor = transforms.ToTensor()\n",
        "\n",
        "        try:\n",
        "            self.brightness = (0.8, 1.2)\n",
        "            self.contrast = (0.8, 1.2)\n",
        "            self.saturation = (0.8, 1.2)\n",
        "            self.hue = (-0.1, 0.1)\n",
        "            transforms.ColorJitter.get_params(self.brightness, self.contrast, self.saturation, self.hue)\n",
        "        except TypeError:\n",
        "            self.brightness = 0.2\n",
        "            self.contrast = 0.2\n",
        "            self.saturation = 0.2\n",
        "            self.hue = 0.1\n",
        "\n",
        "        self.resize = {}\n",
        "        for i in range(self.num_scales):\n",
        "            s = 2 ** i\n",
        "            self.resize[i] = transforms.Resize((self.height // s, self.width // s), interpolation=self.interp)\n",
        "\n",
        "        self.load_depth = self.check_depth()\n",
        "\n",
        "    def preprocess(self, inputs, color_aug, flag=False):\n",
        "        \"\"\"Resize colour images to the required scales and augment if required\n",
        "\n",
        "        We create the color_aug object in advance and apply the same augmentation to all\n",
        "        images in this item. This ensures that all images input to the pose network receive the\n",
        "        same augmentation.\n",
        "        \"\"\"\n",
        "        for k in list(inputs):\n",
        "            frame = inputs[k]\n",
        "            if \"color\" in k:\n",
        "                n, im, i = k\n",
        "                for i in range(self.num_scales):\n",
        "                    inputs[(n, im, i)] = self.resize[i](inputs[(n, im, i - 1)])\n",
        "\n",
        "        for k in list(inputs):\n",
        "            f = inputs[k]\n",
        "            if \"color\" in k:\n",
        "                n, im, i = k\n",
        "                inputs[(n, im, i)] = self.to_tensor(f)\n",
        "                if flag:\n",
        "                    inputs[(n + \"_aug\", im, i)] = color_aug(to_pil_image(f))\n",
        "                else:\n",
        "                    inputs[(n + \"_aug\", im, i)] = self.to_tensor(color_aug(f))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Returns a single training item from the dataset as a dictionary.\n",
        "\n",
        "        Values correspond to torch tensors.\n",
        "        Keys in the dictionary are either strings or tuples:\n",
        "\n",
        "            (\"color\", <frame_id>, <scale>)          for raw colour images,\n",
        "            (\"color_aug\", <frame_id>, <scale>)      for augmented colour images,\n",
        "            (\"K\", scale) or (\"inv_K\", scale)        for camera intrinsics,\n",
        "            \"depth_gt\"                              for ground truth depth maps.\n",
        "\n",
        "        <frame_id> is either:\n",
        "            an integer (e.g. 0, -1, or 1) representing the temporal step relative to 'index'\n",
        "\n",
        "        <scale> is an integer representing the scale of the image relative to the fullsize image:\n",
        "            -1      images at native resolution as loaded from disk\n",
        "            0       images resized to (self.width,      self.height     )\n",
        "            1       images resized to (self.width // 2, self.height // 2)\n",
        "            2       images resized to (self.width // 4, self.height // 4)\n",
        "            3       images resized to (self.width // 8, self.height // 8)\n",
        "        \"\"\"\n",
        "        inputs = {}\n",
        "\n",
        "        # do_color_aug = self.is_train and random.random() > 0.5\n",
        "        do_color_aug = False\n",
        "        # do_flip = self.is_train and random.random() > 0.5\n",
        "        do_flip = False\n",
        "\n",
        "        line = self.filenames[index].split()\n",
        "        folder = line[0]\n",
        "\n",
        "        if len(line) == 3:\n",
        "            frame_index = int(line[1])\n",
        "        else:\n",
        "            frame_index = 0\n",
        "\n",
        "        if len(line) == 3:\n",
        "            side = line[2]\n",
        "        else:\n",
        "            side = None\n",
        "            print(\"Side=None at: \", index)\n",
        "\n",
        "        for i in self.frame_idxs:\n",
        "            inputs[(\"color\", i, -1)] = self.get_color(folder, frame_index + i, side, do_flip)\n",
        "\n",
        "        # adjusting intrinsics to match each scale in the pyramid\n",
        "        for scale in range(self.num_scales):\n",
        "            K = self.K.copy()\n",
        "\n",
        "            K[0, :] *= self.width // (2 ** scale)\n",
        "            K[1, :] *= self.height // (2 ** scale)\n",
        "\n",
        "            inv_K = np.linalg.pinv(K)\n",
        "\n",
        "            inputs[(\"K\", scale)] = torch.from_numpy(K)\n",
        "            inputs[(\"inv_K\", scale)] = torch.from_numpy(inv_K)\n",
        "\n",
        "        if do_color_aug:\n",
        "            color_aug = transforms.ColorJitter.get_params(\n",
        "                self.brightness, self.contrast, self.saturation, self.hue)\n",
        "        else:\n",
        "            color_aug = (lambda x: x)\n",
        "\n",
        "        self.preprocess(inputs, color_aug)\n",
        "\n",
        "        for i in self.frame_idxs:\n",
        "            del inputs[(\"color\", i, -1)]\n",
        "            del inputs[(\"color_aug\", i, -1)]\n",
        "\n",
        "        if self.load_depth:\n",
        "            depth_gt = self.get_depth(folder, frame_index, side, do_flip)\n",
        "            inputs[\"depth_gt\"] = np.expand_dims(depth_gt, 0)\n",
        "            inputs[\"depth_gt\"] = torch.from_numpy(inputs[\"depth_gt\"].astype(np.float32))\n",
        "\n",
        "        return inputs\n",
        "\n",
        "    def get_color(self, folder, frame_index, side, do_flip):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def check_depth(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def get_depth(self, folder, frame_index, side, do_flip):\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NYUv2 Dataset\n"
      ],
      "metadata": {
        "id": "CPZSPazhvh1I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NYUv2Dataset(MonoDataset):\n",
        "    \"\"\"Superclass for different types of NYUv2 dataset loaders\n",
        "    \"\"\"\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(NYUv2Dataset, self).__init__(*args, **kwargs)\n",
        "\n",
        "        # NOTE: Make sure your intrinsics matrix is *normalized* by the original image size.\n",
        "        # To normalize you need to scale the first row by 1 / image_width and the second row\n",
        "        # by 1 / image_height. Monodepth2 assumes a principal point to be exactly centered.\n",
        "        # If your principal point is far from the center you might need to disable the horizontal\n",
        "        # flip augmentation.\n",
        "        self.K = np.array([[0.81, 0, 0.51, 0],\n",
        "                           [0, 2.71, 1.32, 0],\n",
        "                           [0, 0, 1, 0],\n",
        "                           [0, 0, 0, 1]], dtype=np.float32)\n",
        "\n",
        "        self.full_res_shape = (1242, 375)\n",
        "\n",
        "    def check_depth(self):\n",
        "        line = self.filenames[0].split()\n",
        "        scene_name = line[0]\n",
        "        frame_index = int(line[1])\n",
        "\n",
        "        depth_filename = os.path.join(\n",
        "            self.data_path,\n",
        "            scene_name,\n",
        "            \"depth/{:010d}.png\".format(int(frame_index)))\n",
        "\n",
        "        return os.path.isfile(depth_filename)\n",
        "\n",
        "    def get_color(self, folder, frame_index, side, do_flip):\n",
        "        color = self.loader(self.get_image_path(folder, frame_index, side))\n",
        "\n",
        "        if do_flip:\n",
        "            color = color.transpose(pil.FLIP_LEFT_RIGHT)\n",
        "\n",
        "        return color\n",
        "\n",
        "\n",
        "class NYUv2RAWDataset(NYUv2Dataset):\n",
        "    \"\"\"KITTI dataset which loads the original depth maps for ground truth\n",
        "    \"\"\"\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(NYUv2RAWDataset, self).__init__(*args, **kwargs)\n",
        "\n",
        "    def get_image_path(self, folder, frame_index, side):\n",
        "        if frame_index<0:\n",
        "            frame_index = 0\n",
        "        if side==None:\n",
        "            side = \"l\"\n",
        "        f_str = \"{:010d}{}\".format(frame_index, self.img_ext)\n",
        "        if os.path.isfile(os.path.join(self.data_path, folder, \"image\", f_str)):\n",
        "            image_path = os.path.join(self.data_path, folder, \"image\", f_str)\n",
        "        else:\n",
        "            frame_index-=1\n",
        "            f_str = \"{:010d}{}\".format(frame_index, self.img_ext)\n",
        "            image_path = os.path.join(self.data_path, folder, \"image\", f_str)\n",
        "        return image_path\n",
        "\n",
        "    def get_depth(self, folder, frame_index, side, do_flip):\n",
        "        if frame_index<0:\n",
        "            frame_index = 0\n",
        "        depth_filename = os.path.join(self.data_path, folder,\n",
        "            \"depth/{:010d}.png\".format(int(frame_index)))\n",
        "\n",
        "        if os.path.isfile(depth_filename)==False:\n",
        "            frame_index-=1\n",
        "            depth_filename = os.path.join(self.data_path, folder,\n",
        "                \"depth/{:010d}.png\".format(int(frame_index)))\n",
        "\n",
        "        if side==None:\n",
        "            side = \"l\"\n",
        "        depth_gt = cv2.imread(depth_filename, cv2.IMREAD_UNCHANGED)\n",
        "        depth_gt = skimage.transform.resize(\n",
        "            depth_gt, self.full_res_shape[::-1], order=0, preserve_range=True, mode='constant')\n",
        "\n",
        "        if do_flip:\n",
        "            depth_gt = np.fliplr(depth_gt)\n",
        "\n",
        "        return depth_gt"
      ],
      "metadata": {
        "id": "DBEaD7b1vgiF"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_7b5D-Iuzmw"
      },
      "source": [
        "# Building Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kDj62Ah6x7Q"
      },
      "source": [
        "Building layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "ycH1NRDL0YCV"
      },
      "outputs": [],
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    \"\"\"Layer to perform a convolution followed by ELU\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ConvBlock, self).__init__()\n",
        "\n",
        "        self.conv = Conv3x3(in_channels, out_channels)\n",
        "        self.nonlin = nn.ELU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)\n",
        "        out = self.nonlin(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Conv3x3(nn.Module):\n",
        "    \"\"\"Layer to pad and convolve input\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, use_refl=True):\n",
        "        super(Conv3x3, self).__init__()\n",
        "\n",
        "        if use_refl:\n",
        "            self.pad = nn.ReflectionPad2d(1)\n",
        "        else:\n",
        "            self.pad = nn.ZeroPad2d(1)\n",
        "        self.conv = nn.Conv2d(int(in_channels), int(out_channels), 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.pad(x)\n",
        "        out = self.conv(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class BackprojectDepth(nn.Module):\n",
        "    \"\"\"Layer to transform a depth image into a point cloud\n",
        "    \"\"\"\n",
        "    def __init__(self, batch_size, height, width):\n",
        "        super(BackprojectDepth, self).__init__()\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "\n",
        "        meshgrid = np.meshgrid(range(self.width), range(self.height), indexing='xy')\n",
        "        self.id_coords = np.stack(meshgrid, axis=0).astype(np.float32)\n",
        "        self.id_coords = nn.Parameter(torch.from_numpy(self.id_coords),\n",
        "                                      requires_grad=False)\n",
        "\n",
        "        self.ones = nn.Parameter(torch.ones(self.batch_size, 1, self.height * self.width),\n",
        "                                 requires_grad=False)\n",
        "\n",
        "        self.pix_coords = torch.unsqueeze(torch.stack(\n",
        "            [self.id_coords[0].view(-1), self.id_coords[1].view(-1)], 0), 0)\n",
        "        self.pix_coords = self.pix_coords.repeat(batch_size, 1, 1)\n",
        "        self.pix_coords = nn.Parameter(torch.cat([self.pix_coords, self.ones], 1),\n",
        "                                       requires_grad=False)\n",
        "\n",
        "    def forward(self, depth, inv_K):\n",
        "        cam_points = torch.matmul(inv_K[:, :3, :3], self.pix_coords)\n",
        "        cam_points = depth.view(self.batch_size, 1, -1) * cam_points\n",
        "        cam_points = torch.cat([cam_points, self.ones], 1)\n",
        "\n",
        "        return cam_points\n",
        "\n",
        "\n",
        "class Project3D(nn.Module):\n",
        "    \"\"\"Layer which projects 3D points into a camera with intrinsics K and at position T\n",
        "    \"\"\"\n",
        "    def __init__(self, batch_size, height, width, eps=1e-7):\n",
        "        super(Project3D, self).__init__()\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, points, K, T):\n",
        "        P = torch.matmul(K, T)[:, :3, :]\n",
        "\n",
        "        cam_points = torch.matmul(P, points)\n",
        "\n",
        "        pix_coords = cam_points[:, :2, :] / (cam_points[:, 2, :].unsqueeze(1) + self.eps)\n",
        "        pix_coords = pix_coords.view(self.batch_size, 2, self.height, self.width)\n",
        "        pix_coords = pix_coords.permute(0, 2, 3, 1)\n",
        "        pix_coords[..., 0] /= self.width - 1\n",
        "        pix_coords[..., 1] /= self.height - 1\n",
        "        pix_coords = (pix_coords - 0.5) * 2\n",
        "        return pix_coords\n",
        "\n",
        "\n",
        "class SSIM(nn.Module):\n",
        "    \"\"\"Layer to compute the SSIM loss between a pair of images\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(SSIM, self).__init__()\n",
        "        self.mu_x_pool   = nn.AvgPool2d(3, 1)\n",
        "        self.mu_y_pool   = nn.AvgPool2d(3, 1)\n",
        "        self.sig_x_pool  = nn.AvgPool2d(3, 1)\n",
        "        self.sig_y_pool  = nn.AvgPool2d(3, 1)\n",
        "        self.sig_xy_pool = nn.AvgPool2d(3, 1)\n",
        "\n",
        "        self.refl = nn.ReflectionPad2d(1)\n",
        "\n",
        "        self.C1 = 0.01 ** 2\n",
        "        self.C2 = 0.03 ** 2\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        x = self.refl(x)\n",
        "        y = self.refl(y)\n",
        "\n",
        "        mu_x = self.mu_x_pool(x)\n",
        "        mu_y = self.mu_y_pool(y)\n",
        "\n",
        "        sigma_x  = self.sig_x_pool(x ** 2) - mu_x ** 2\n",
        "        sigma_y  = self.sig_y_pool(y ** 2) - mu_y ** 2\n",
        "        sigma_xy = self.sig_xy_pool(x * y) - mu_x * mu_y\n",
        "\n",
        "        SSIM_n = (2 * mu_x * mu_y + self.C1) * (2 * sigma_xy + self.C2)\n",
        "        SSIM_d = (mu_x ** 2 + mu_y ** 2 + self.C1) * (sigma_x + sigma_y + self.C2)\n",
        "\n",
        "        return torch.clamp((1 - SSIM_n / SSIM_d) / 2, 0, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6prlUUg16x7R"
      },
      "source": [
        "Building network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "R0i-OSQLuzDl"
      },
      "outputs": [],
      "source": [
        "class DepthDecoder(nn.Module):\n",
        "    def __init__(self, num_ch_enc, scales=range(4), num_output_channels=1, use_skips=True):\n",
        "        super(DepthDecoder, self).__init__()\n",
        "\n",
        "        self.num_output_channels = num_output_channels\n",
        "        self.use_skips = use_skips\n",
        "        self.upsample_mode = 'nearest'\n",
        "        self.scales = scales\n",
        "\n",
        "        self.num_ch_enc = num_ch_enc\n",
        "        self.num_ch_dec = np.array([16, 32, 64, 128, 256])\n",
        "\n",
        "        # decoder\n",
        "        self.convs = OrderedDict()\n",
        "        for i in range(4, -1, -1):\n",
        "            # upconv_0\n",
        "            num_ch_in = self.num_ch_enc[-1] if i == 4 else self.num_ch_dec[i + 1]\n",
        "            num_ch_out = self.num_ch_dec[i]\n",
        "            self.convs[(\"upconv\", i, 0)] = ConvBlock(num_ch_in, num_ch_out)\n",
        "\n",
        "            # upconv_1\n",
        "            num_ch_in = self.num_ch_dec[i]\n",
        "            if self.use_skips and i > 0:\n",
        "                num_ch_in += self.num_ch_enc[i - 1]\n",
        "            num_ch_out = self.num_ch_dec[i]\n",
        "            self.convs[(\"upconv\", i, 1)] = ConvBlock(num_ch_in, num_ch_out)\n",
        "\n",
        "        for s in self.scales:\n",
        "            self.convs[(\"dispconv\", s)] = Conv3x3(self.num_ch_dec[s], self.num_output_channels)\n",
        "\n",
        "        self.decoder = nn.ModuleList(list(self.convs.values()))\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input_features):\n",
        "        self.outputs = {}\n",
        "\n",
        "        # decoder\n",
        "        x = input_features[-1]\n",
        "        for i in range(4, -1, -1):\n",
        "            x = self.convs[(\"upconv\", i, 0)](x)\n",
        "            x = [upsample(x)]\n",
        "            if self.use_skips and i > 0:\n",
        "                x += [input_features[i - 1]]\n",
        "            x = torch.cat(x, 1)\n",
        "            x = self.convs[(\"upconv\", i, 1)](x)\n",
        "            if i in self.scales:\n",
        "                self.outputs[(\"disp\", i)] = self.sigmoid(self.convs[(\"dispconv\", i)](x))\n",
        "\n",
        "        return self.outputs\n",
        "\n",
        "\n",
        "class PoseDecoder(nn.Module):\n",
        "    def __init__(self, num_ch_enc, num_input_features, num_frames_to_predict_for=None, stride=1):\n",
        "        super(PoseDecoder, self).__init__()\n",
        "\n",
        "        self.num_ch_enc = num_ch_enc\n",
        "        self.num_input_features = num_input_features\n",
        "\n",
        "        if num_frames_to_predict_for is None:\n",
        "            num_frames_to_predict_for = num_input_features - 1\n",
        "        self.num_frames_to_predict_for = num_frames_to_predict_for\n",
        "\n",
        "        self.convs = OrderedDict()\n",
        "        self.convs[(\"squeeze\")] = nn.Conv2d(self.num_ch_enc[-1], 256, 1)\n",
        "        self.convs[(\"pose\", 0)] = nn.Conv2d(num_input_features * 256, 256, 3, stride, 1)\n",
        "        self.convs[(\"pose\", 1)] = nn.Conv2d(256, 256, 3, stride, 1)\n",
        "        self.convs[(\"pose\", 2)] = nn.Conv2d(256, 6 * num_frames_to_predict_for, 1)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.net = nn.ModuleList(list(self.convs.values()))\n",
        "\n",
        "    def forward(self, input_features):\n",
        "        last_features = [f[-1] for f in input_features]\n",
        "\n",
        "        cat_features = [self.relu(self.convs[\"squeeze\"](f)) for f in last_features]\n",
        "        cat_features = torch.cat(cat_features, 1)\n",
        "\n",
        "        out = cat_features\n",
        "        for i in range(3):\n",
        "            out = self.convs[(\"pose\", i)](out)\n",
        "            if i != 2:\n",
        "                out = self.relu(out)\n",
        "\n",
        "        out = out.mean(3).mean(2)\n",
        "\n",
        "        out = 0.01 * out.view(-1, self.num_frames_to_predict_for, 1, 6)\n",
        "\n",
        "        axisangle = out[..., :3]\n",
        "        translation = out[..., 3:]\n",
        "\n",
        "        return axisangle, translation\n",
        "\n",
        "\n",
        "class ResNetMultiImageInput(models.ResNet):\n",
        "    \"\"\"Constructs a resnet model with varying number of input images.\"\"\"\n",
        "    def __init__(self, block, layers, num_classes=1000, num_input_images=1):\n",
        "        super(ResNetMultiImageInput, self).__init__(block, layers)\n",
        "        self.inplanes = 64\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            num_input_images * 3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "def resnet_multiimage_input(num_layers, pretrained=False, num_input_images=1):\n",
        "    \"\"\"Constructs a ResNet model.\n",
        "    Args:\n",
        "        num_layers (int): Number of resnet layers. Must be 18 or 50\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        num_input_images (int): Number of frames stacked as input\n",
        "    \"\"\"\n",
        "    assert num_layers in [18, 50], \"Can only run with 18 or 50 layer resnet\"\n",
        "    blocks = {18: [2, 2, 2, 2], 50: [3, 4, 6, 3]}[num_layers]\n",
        "    block_type = {18: models.resnet.BasicBlock, 50: models.resnet.Bottleneck}[num_layers]\n",
        "    model = ResNetMultiImageInput(block_type, blocks, num_input_images=num_input_images)\n",
        "\n",
        "    if pretrained:\n",
        "        loaded = model_zoo.load_url(models.resnet.model_urls['resnet{}'.format(num_layers)])\n",
        "        loaded['conv1.weight'] = torch.cat(\n",
        "            [loaded['conv1.weight']] * num_input_images, 1) / num_input_images\n",
        "        model.load_state_dict(loaded)\n",
        "    return model\n",
        "\n",
        "\n",
        "class ResnetEncoder(nn.Module):\n",
        "    \"\"\"Pytorch module for a resnet encode\"\"\"\n",
        "    def __init__(self, num_layers, pretrained, num_input_images=1):\n",
        "        super(ResnetEncoder, self).__init__()\n",
        "\n",
        "        self.num_ch_enc = np.array([64, 64, 128, 256, 512])\n",
        "\n",
        "        resnets = {18: models.resnet18,\n",
        "                   50: models.resnet50}\n",
        "\n",
        "        if num_layers not in resnets:\n",
        "            raise ValueError(\"{} is not a valid number of resnet layers\".format(num_layers))\n",
        "\n",
        "        if num_input_images > 1:\n",
        "            self.encoder = resnet_multiimage_input(num_layers, pretrained, num_input_images)\n",
        "        else:\n",
        "            self.encoder = resnets[num_layers](pretrained)\n",
        "\n",
        "        if num_layers > 34:\n",
        "            self.num_ch_enc[1:] *= 4\n",
        "\n",
        "    def forward(self, input_image):\n",
        "        self.features = []\n",
        "        x = (input_image - 0.45) / 0.225\n",
        "        x = self.encoder.conv1(x)\n",
        "        x = self.encoder.bn1(x)\n",
        "        self.features.append(self.encoder.relu(x))\n",
        "        self.features.append(self.encoder.layer1(self.encoder.maxpool(self.features[-1])))\n",
        "        self.features.append(self.encoder.layer2(self.features[-1]))\n",
        "        self.features.append(self.encoder.layer3(self.features[-1]))\n",
        "        self.features.append(self.encoder.layer4(self.features[-1]))\n",
        "\n",
        "        return self.features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R99pe_8q6x7R"
      },
      "source": [
        "Monodepth forward model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "wmIpknaC6x7R"
      },
      "outputs": [],
      "source": [
        "class Monodepth:\n",
        "    def __init__(self, num_layers=18, pretrained=False):\n",
        "        self.encoder = ResnetEncoder(num_layers, pretrained)\n",
        "        self.depth_decoder = DepthDecoder(num_ch_enc=self.encoder.num_ch_enc, scales=range(4))\n",
        "\n",
        "    def load_checkpoint(self, model_path):\n",
        "        encoder_path = os.path.join(model_path, \"encoder.pth\")\n",
        "        depth_decoder_path = os.path.join(model_path, \"depth.pth\")\n",
        "\n",
        "        self.loaded_dict_enc = torch.load(encoder_path, map_location='cpu')\n",
        "        self.filtered_dict_enc = {k: v for k, v in self.loaded_dict_enc.items() if k in self.encoder.state_dict()}\n",
        "        self.encoder.load_state_dict(self.filtered_dict_enc)\n",
        "        self.loaded_dict = torch.load(depth_decoder_path, map_location='cpu')\n",
        "        self.depth_decoder.load_state_dict(self.loaded_dict)\n",
        "        self.feed_height = self.loaded_dict_enc['height']\n",
        "        self.feed_width = self.loaded_dict_enc['width']\n",
        "\n",
        "        self.encoder.eval()\n",
        "        self.depth_decoder.eval();\n",
        "\n",
        "    def forward(self, img):\n",
        "        with torch.no_grad():\n",
        "            features = self.encoder(img)\n",
        "            outputs = self.depth_decoder(features)\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nq_fLSAI0nge"
      },
      "source": [
        "# Trainning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9AmsZWG6x7R"
      },
      "source": [
        "Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "dpoTSgZa6x7R"
      },
      "outputs": [],
      "source": [
        "class Hyperparameters:\n",
        "    def __init__(self, height=192, width=640, save_freq=1, showimg_freq=1, batch_size=1,\n",
        "                 log_path=\"\", checkpoint_dir=\"\", dataset=\"\", data_path=\"\", split_dir=\"\",\n",
        "                 learning_rate=1e-4, scheduler_step_size=15, num_epochs=1, num_workers=2,\n",
        "                 module_load=[\"encoder\", \"depth\", \"pose_encoder\", \"pose\"], pretrained=False,\n",
        "                 num_pose_frames=2, frame_ids=[0,-1,1], scales=[0,1,2,3], img_type=\".png\",\n",
        "                 log_frequency=250, min_depth=0.1, max_depth=100.0, disparity_smoothness=1e-3):\n",
        "        #need to load: log_path, dataset, data_path, split_dir\n",
        "        self.frame_ids = frame_ids if frame_ids[0]==0 else [0,-1,1]\n",
        "        self.disparity_smoothness = disparity_smoothness\n",
        "        self.scheduler_step_size = scheduler_step_size\n",
        "        self.height = height if height%32==0 else 192\n",
        "        self.width = width if width%32==0 else 640\n",
        "        self.num_pose_frames = num_pose_frames\n",
        "        self.checkpoint_dir = checkpoint_dir\n",
        "        self.log_frequency = log_frequency\n",
        "        self.learning_rate = learning_rate\n",
        "        self.showimg_freq = showimg_freq\n",
        "        self.num_workers = num_workers\n",
        "        self.module_load = module_load\n",
        "        self.pretrained = pretrained\n",
        "        self.batch_size = batch_size\n",
        "        self.num_epochs = num_epochs\n",
        "        self.data_path =  data_path\n",
        "        self.split_dir = split_dir\n",
        "        self.min_depth = min_depth\n",
        "        self.max_depth = max_depth\n",
        "        self.save_freq = save_freq\n",
        "        self.log_path = log_path\n",
        "        self.img_type = img_type\n",
        "        self.dataset = dataset\n",
        "        self.device = DEVICE\n",
        "        self.scales = scales\n",
        "        self.num_layers = 18\n",
        "        self.v1_multiscale = True\n",
        "        self.avg_reprojection = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsUzdQ2e6x7S"
      },
      "source": [
        "Train process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "pJ3tg07o0zeL"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    def __init__(self, hyperparameters):\n",
        "        # prepare attributes\n",
        "        self.hp = hyperparameters\n",
        "        self.device = torch.device(self.hp.device)\n",
        "        self.models = {}\n",
        "        self.writers = {}\n",
        "        self.project_3d = {}\n",
        "        self.backproject_depth = {}\n",
        "        self.parameters_to_train = []\n",
        "        self.log_path = self.hp.log_path\n",
        "        self.save_freq = self.hp.save_freq\n",
        "        self.num_scales = len(self.hp.scales)\n",
        "        self.showimg_freq = self.hp.showimg_freq\n",
        "        self.num_input_frames = len(self.hp.frame_ids)\n",
        "        self.num_pose_frames = self.hp.num_pose_frames\n",
        "\n",
        "        # prepare network\n",
        "        self.models[\"encoder\"] = ResnetEncoder(self.hp.num_layers, self.hp.pretrained)\n",
        "        self.models[\"depth\"] = DepthDecoder(self.models[\"encoder\"].num_ch_enc, self.hp.scales)\n",
        "        self.models[\"pose_encoder\"] = ResnetEncoder(self.hp.num_layers, self.hp.pretrained, num_input_images=self.num_pose_frames)\n",
        "        self.models[\"pose\"] = PoseDecoder(self.models[\"pose_encoder\"].num_ch_enc, num_input_features=1, num_frames_to_predict_for=2)\n",
        "        self.models[\"encoder\"].to(self.device)\n",
        "        self.models[\"depth\"].to(self.device)\n",
        "        self.models[\"pose_encoder\"].to(self.device)\n",
        "        self.models[\"pose\"].to(self.device)\n",
        "        self.parameters_to_train += list(self.models[\"encoder\"].parameters())\n",
        "        self.parameters_to_train += list(self.models[\"depth\"].parameters())\n",
        "        self.parameters_to_train += list(self.models[\"pose_encoder\"].parameters())\n",
        "        self.parameters_to_train += list(self.models[\"pose\"].parameters())\n",
        "\n",
        "        self.ssim = SSIM()\n",
        "        self.ssim.to(self.device)\n",
        "        self.model_optimizer = optim.Adam(self.parameters_to_train, self.hp.learning_rate)\n",
        "        self.model_lr_scheduler = optim.lr_scheduler.StepLR(self.model_optimizer, self.hp.scheduler_step_size, 0.1)\n",
        "        self.depth_metric_names = [\"de/abs_rel\", \"de/sq_rel\", \"de/rms\", \"de/log_rms\", \"da/a1\", \"da/a2\", \"da/a3\"]\n",
        "\n",
        "        if self.hp.checkpoint_dir!=\"\":\n",
        "            self.load_model()\n",
        "\n",
        "        # projection\n",
        "        for scale in self.hp.scales:\n",
        "            h = self.hp.height // (2 ** scale)\n",
        "            w = self.hp.width // (2 ** scale)\n",
        "            self.backproject_depth[scale] = BackprojectDepth(self.hp.batch_size, h, w)\n",
        "            self.backproject_depth[scale].to(self.device)\n",
        "            self.project_3d[scale] = Project3D(self.hp.batch_size, h, w)\n",
        "            self.project_3d[scale].to(self.device)\n",
        "\n",
        "        # prepare data\n",
        "        datasets_dict = {\"nyuv2\": NYUv2RAWDataset}\n",
        "        self.dataset = datasets_dict[self.hp.dataset]\n",
        "        fpath = os.path.join(self.hp.split_dir, \"{}_files.txt\")\n",
        "        train_filenames = readlines(fpath.format(\"train\"))\n",
        "        val_filenames = readlines(fpath.format(\"val\"))\n",
        "        test_filenames = readlines(fpath.format(\"test\"))\n",
        "        img_ext = self.hp.img_type\n",
        "        num_train_samples = len(train_filenames)\n",
        "        self.num_total_steps = num_train_samples // self.hp.batch_size * self.hp.num_epochs\n",
        "\n",
        "        train_dataset = self.dataset(self.hp.data_path, train_filenames, self.hp.height, self.hp.width, self.hp.frame_ids, 4, is_train=True, img_ext=img_ext)\n",
        "        val_dataset = self.dataset(self.hp.data_path, val_filenames, self.hp.height, self.hp.width, self.hp.frame_ids, 4, is_train=False, img_ext=img_ext)\n",
        "        test_dataset = self.dataset(self.hp.data_path, test_filenames, self.hp.height, self.hp.width, self.hp.frame_ids, 4, is_train=False, img_ext=img_ext)\n",
        "        self.train_loader = DataLoader(train_dataset, self.hp.batch_size, True, num_workers=self.hp.num_workers, pin_memory=True, drop_last=True)\n",
        "        self.val_loader = DataLoader(val_dataset, self.hp.batch_size, True, num_workers=self.hp.num_workers, pin_memory=True, drop_last=True)\n",
        "        self.test_loader = DataLoader(test_dataset, self.hp.batch_size, True, num_workers=self.hp.num_workers, pin_memory=True, drop_last=True)\n",
        "        self.val_iter = iter(self.val_loader)\n",
        "\n",
        "\n",
        "        print(\"There are {:d} training items, {:d} validation items and {:d} test items\\n\".format(len(train_dataset), len(val_dataset), len(test_dataset)))\n",
        "\n",
        "    def set_train(self):\n",
        "        \"\"\"Convert all models to training mode\"\"\"\n",
        "        for m in self.models.values():\n",
        "            m.train()\n",
        "\n",
        "    def set_eval(self):\n",
        "        \"\"\"Convert all models to testing/evaluation mode\"\"\"\n",
        "        for m in self.models.values():\n",
        "            m.eval()\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Run the entire training pipeline\"\"\"\n",
        "        self.epoch = 0\n",
        "        self.step = 0\n",
        "        self.start_time = time.time()\n",
        "        for self.epoch in range(self.hp.num_epochs):\n",
        "            self.run_epoch()\n",
        "            self.save_model()\n",
        "            print(\"save!\")\n",
        "\n",
        "    def run_epoch(self):\n",
        "        \"\"\"Run a single epoch of training and validation\"\"\"\n",
        "        self.model_lr_scheduler.step()\n",
        "        print(\"Training\")\n",
        "        self.set_train()\n",
        "\n",
        "        for batch_idx, inputs in enumerate(self.train_loader):\n",
        "            print(\"|--------------------|\")\n",
        "            print(\"Batch: \", batch_idx)\n",
        "            for k in inputs:\n",
        "                inputs[k] = inputs[k].to(self.device)\n",
        "            outputs, losses = self.process_batch(inputs)\n",
        "            self.model_optimizer.zero_grad()\n",
        "            losses[\"loss\"].backward()\n",
        "            self.model_optimizer.step()\n",
        "            if batch_idx % self.hp.log_frequency == 0:\n",
        "                if \"depth_gt\" in inputs:\n",
        "                    self.compute_depth_losses(inputs, outputs, losses)\n",
        "                show_images(outputs)\n",
        "                self.save_model()\n",
        "                print(\"save!\")\n",
        "                self.val()\n",
        "\n",
        "            self.step += 1\n",
        "            print(\"Done Batch!\")\n",
        "\n",
        "    def testing(self):\n",
        "        print(\"Testing\")\n",
        "        avr = {}\n",
        "        num  = 0\n",
        "        for i, metric in enumerate(self.depth_metric_names):\n",
        "            avr[metric] = 0\n",
        "        for batch_idx, inputs in enumerate(self.test_loader):\n",
        "            print(\"|--------------------|\")\n",
        "            print(\"test: \", batch_idx)\n",
        "            num+=1\n",
        "            for k in inputs:\n",
        "                inputs[k] = inputs[k].to(self.device)\n",
        "            outputs, losses = self.process_batch(inputs)\n",
        "            self.compute_depth_losses(inputs, outputs, losses)\n",
        "            show_rgb(inputs[\"color_aug\", 0, 0])\n",
        "            show_images(outputs)\n",
        "            for i, metric in enumerate(self.depth_metric_names):\n",
        "                print(metric, \":  \", losses[metric])\n",
        "                avr[metric]+=losses[metric]\n",
        "                print(metric, \"average: \", avr[metric]/num)\n",
        "            print(\"Done!\")\n",
        "        print('Final result:')\n",
        "        for i, metric in enumerate(self.depth_metric_names):\n",
        "            print(metric, \":  \", avr[metric]/num)\n",
        "\n",
        "    def process_batch(self, inputs):\n",
        "        \"\"\"Pass a minibatch through the network and generate images and losses\"\"\"\n",
        "        features = self.models[\"encoder\"](inputs[\"color_aug\", 0, 0])\n",
        "        outputs = self.models[\"depth\"](features)\n",
        "        outputs.update(self.predict_poses(inputs, features))\n",
        "        self.generate_images_pred(inputs, outputs)\n",
        "        losses = self.compute_losses(inputs, outputs)\n",
        "\n",
        "        return outputs, losses\n",
        "\n",
        "    def predict_poses(self, inputs, features):\n",
        "        \"\"\"Predict poses between input frames for monocular sequences.\"\"\"\n",
        "        outputs = {}\n",
        "        if self.num_pose_frames == 2:\n",
        "            # In this setting, we compute the pose to each source frame\n",
        "            pose_feats = {f_i: inputs[\"color_aug\", f_i, 0] for f_i in self.hp.frame_ids}\n",
        "\n",
        "            for f_i in self.hp.frame_ids[1:]:\n",
        "                # To maintain ordering we always pass frames in temporal order\n",
        "                if f_i < 0:\n",
        "                    pose_inputs = [pose_feats[f_i], pose_feats[0]]\n",
        "                else:\n",
        "                    pose_inputs = [pose_feats[0], pose_feats[f_i]]\n",
        "                pose_inputs = [self.models[\"pose_encoder\"](torch.cat(pose_inputs, 1))]\n",
        "                axisangle, translation = self.models[\"pose\"](pose_inputs)\n",
        "                outputs[(\"axisangle\", 0, f_i)] = axisangle\n",
        "                outputs[(\"translation\", 0, f_i)] = translation\n",
        "                outputs[(\"cam_T_cam\", 0, f_i)] = transformation_from_parameters(axisangle[:, 0], translation[:, 0], invert=(f_i < 0))\n",
        "\n",
        "        else:\n",
        "            # Here we input all frames to the pose net (and predict all poses) together\n",
        "            pose_inputs = torch.cat([inputs[(\"color_aug\", i, 0)] for i in self.hp.frame_ids], 1)\n",
        "            pose_inputs = [self.models[\"pose_encoder\"](pose_inputs)]\n",
        "            axisangle, translation = self.models[\"pose\"](pose_inputs)\n",
        "            for i, f_i in enumerate(self.hp.frame_ids[1:]):\n",
        "                outputs[(\"axisangle\", 0, f_i)] = axisangle\n",
        "                outputs[(\"translation\", 0, f_i)] = translation\n",
        "                outputs[(\"cam_T_cam\", 0, f_i)] = transformation_from_parameters(axisangle[:, i], translation[:, i])\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def val(self):\n",
        "        \"\"\"Validate the model on a single minibatch\"\"\"\n",
        "        self.set_eval()\n",
        "        try:\n",
        "            inputs = self.val_iter.__next__()\n",
        "        except StopIteration:\n",
        "            self.val_iter = iter(self.val_loader)\n",
        "            inputs = self.val_iter.__next__()\n",
        "\n",
        "        for k in inputs:\n",
        "            inputs[k] = inputs[k].to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs, losses = self.process_batch(inputs)\n",
        "            if \"depth_gt\" in inputs:\n",
        "                self.compute_depth_losses(inputs, outputs, losses)\n",
        "            del inputs, outputs, losses\n",
        "        self.set_train()\n",
        "\n",
        "    def generate_images_pred(self, inputs, outputs):\n",
        "        \"\"\"Generate the warped (reprojected) color images for a minibatch.\n",
        "        Generated images are saved into the `outputs` dictionary.\"\"\"\n",
        "        for scale in self.hp.scales:\n",
        "            disp = outputs[(\"disp\", scale)]\n",
        "            source_scale = scale\n",
        "            _, depth = disp_to_depth(disp, self.hp.min_depth, self.hp.max_depth)\n",
        "            outputs[(\"depth\", 0, scale)] = depth\n",
        "\n",
        "            for i, frame_id in enumerate(self.hp.frame_ids[1:]):\n",
        "                T = outputs[(\"cam_T_cam\", 0, frame_id)]\n",
        "                cam_points = self.backproject_depth[source_scale](depth, inputs[(\"inv_K\", source_scale)])\n",
        "                pix_coords = self.project_3d[source_scale](cam_points, inputs[(\"K\", source_scale)], T)\n",
        "\n",
        "                outputs[(\"sample\", frame_id, scale)] = pix_coords\n",
        "                outputs[(\"color\", frame_id, scale)] = F.grid_sample(inputs[(\"color\", frame_id, source_scale)], outputs[(\"sample\", frame_id, scale)], padding_mode=\"border\")\n",
        "                outputs[(\"color_identity\", frame_id, scale)] = inputs[(\"color\", frame_id, source_scale)]\n",
        "\n",
        "    def compute_reprojection_loss(self, pred, target):\n",
        "        \"\"\"Computes reprojection loss between a batch of predicted and target images\n",
        "        \"\"\"\n",
        "        abs_diff = torch.abs(target - pred)\n",
        "        l1_loss = abs_diff.mean(1, True)\n",
        "        ssim_loss = self.ssim(pred, target).mean(1, True)\n",
        "        reprojection_loss = 0.85 * ssim_loss + 0.15 * l1_loss\n",
        "\n",
        "        return reprojection_loss\n",
        "\n",
        "    def compute_losses(self, inputs, outputs):\n",
        "        \"\"\"Compute the reprojection and smoothness losses for a minibatch\"\"\"\n",
        "        losses = {}\n",
        "        total_loss = 0\n",
        "        for scale in self.hp.scales:\n",
        "            loss = 0\n",
        "            reprojection_losses = []\n",
        "            source_scale = scale\n",
        "            disp = outputs[(\"disp\", scale)]\n",
        "            color = inputs[(\"color\", 0, scale)]\n",
        "            target = inputs[(\"color\", 0, source_scale)]\n",
        "\n",
        "            for frame_id in self.hp.frame_ids[1:]:\n",
        "                pred = outputs[(\"color\", frame_id, scale)]\n",
        "                reprojection_losses.append(self.compute_reprojection_loss(pred, target))\n",
        "\n",
        "            reprojection_losses = torch.cat(reprojection_losses, 1)\n",
        "\n",
        "\n",
        "            identity_reprojection_losses = []\n",
        "            for frame_id in self.hp.frame_ids[1:]:\n",
        "                pred = inputs[(\"color\", frame_id, source_scale)]\n",
        "                identity_reprojection_losses.append(\n",
        "                    self.compute_reprojection_loss(pred, target))\n",
        "\n",
        "            identity_reprojection_losses = torch.cat(identity_reprojection_losses, 1)\n",
        "\n",
        "\n",
        "            # save both images, and do min all at once below\n",
        "            identity_reprojection_loss = identity_reprojection_losses\n",
        "\n",
        "\n",
        "            reprojection_loss = reprojection_losses\n",
        "\n",
        "\n",
        "            # add random numbers to break ties\n",
        "            identity_reprojection_loss += torch.randn(identity_reprojection_loss.shape, device=self.device) * 0.00001\n",
        "            combined = torch.cat((identity_reprojection_loss, reprojection_loss), dim=1)\n",
        "\n",
        "            if combined.shape[1] == 1:\n",
        "                to_optimise = combined\n",
        "            else:\n",
        "                to_optimise, idxs = torch.min(combined, dim=1)\n",
        "\n",
        "            outputs[\"identity_selection/{}\".format(scale)] = (\n",
        "                idxs > identity_reprojection_loss.shape[1] - 1).float()\n",
        "\n",
        "            loss += to_optimise.mean()\n",
        "\n",
        "            mean_disp = disp.mean(2, True).mean(3, True)\n",
        "            norm_disp = disp / (mean_disp + 1e-7)\n",
        "            smooth_loss = get_smooth_loss(norm_disp, color)\n",
        "\n",
        "            loss += self.hp.disparity_smoothness * smooth_loss / (2 ** scale)\n",
        "            total_loss += loss\n",
        "            losses[\"loss/{}\".format(scale)] = loss\n",
        "\n",
        "        total_loss /= self.num_scales\n",
        "        losses[\"loss\"] = total_loss\n",
        "        return losses\n",
        "\n",
        "    def compute_depth_losses(self, inputs, outputs, losses):\n",
        "        \"\"\"Compute depth metrics, to allow monitoring during training\n",
        "\n",
        "        This isn't particularly accurate as it averages over the entire batch,\n",
        "        so is only used to give an indication of validation performance\"\"\"\n",
        "        depth_pred = outputs[(\"depth\", 0, 0)]\n",
        "        depth_pred = torch.clamp(F.interpolate(depth_pred, [375, 1242], mode=\"bilinear\", align_corners=False), 1e-3, 80)\n",
        "        depth_pred = depth_pred.detach()\n",
        "\n",
        "        depth_gt = inputs[\"depth_gt\"]\n",
        "        mask = depth_gt > 0\n",
        "\n",
        "        # garg/eigen crop\n",
        "        crop_mask = torch.zeros_like(mask)\n",
        "        crop_mask[:, :, 153:371, 44:1197] = 1\n",
        "        mask = mask * crop_mask\n",
        "\n",
        "        depth_gt = depth_gt[mask]\n",
        "        depth_pred = depth_pred[mask]\n",
        "        depth_pred *= torch.median(depth_gt) / torch.median(depth_pred)\n",
        "\n",
        "        depth_pred = torch.clamp(depth_pred, min=1e-3, max=80)\n",
        "\n",
        "        depth_errors = compute_depth_errors(depth_gt, depth_pred)\n",
        "\n",
        "        for i, metric in enumerate(self.depth_metric_names):\n",
        "            losses[metric] = np.array(depth_errors[i].cpu())\n",
        "\n",
        "    def save_model(self):\n",
        "        \"\"\"Save model weights to disk\n",
        "        \"\"\"\n",
        "        save_folder = os.path.join(self.log_path, \"models\", \"weights_{}\".format(self.epoch))\n",
        "        if not os.path.exists(save_folder):\n",
        "            os.makedirs(save_folder)\n",
        "\n",
        "        for model_name, model in self.models.items():\n",
        "            save_path = os.path.join(save_folder, \"{}.pth\".format(model_name))\n",
        "            to_save = model.state_dict()\n",
        "            if model_name == 'encoder':\n",
        "                # save the sizes - these are needed at prediction time\n",
        "                to_save['height'] = self.hp.height\n",
        "                to_save['width'] = self.hp.width\n",
        "            torch.save(to_save, save_path)\n",
        "\n",
        "        save_path = os.path.join(save_folder, \"{}.pth\".format(\"adam\"))\n",
        "        torch.save(self.model_optimizer.state_dict(), save_path)\n",
        "\n",
        "    def load_model(self):\n",
        "        \"\"\"Load model(s) from disk\n",
        "        \"\"\"\n",
        "        print(\"Loading model from \"+self.hp.checkpoint_dir)\n",
        "        for n in self.hp.module_load:\n",
        "            path = os.path.join(self.hp.checkpoint_dir, \"{}.pth\".format(n))\n",
        "            model_dict = self.models[n].state_dict()\n",
        "            pretrained_dict = torch.load(path, map_location=torch.device(self.device))\n",
        "            pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
        "            model_dict.update(pretrained_dict)\n",
        "            self.models[n].load_state_dict(model_dict)\n",
        "\n",
        "        # loading adam state\n",
        "        optimizer_load_path = os.path.join(self.hp.checkpoint_dir, \"adam.pth\")\n",
        "        if os.path.isfile(optimizer_load_path):\n",
        "            optimizer_dict = torch.load(optimizer_load_path, map_location=torch.device(self.device))\n",
        "            self.model_optimizer.load_state_dict(optimizer_dict)\n",
        "        print(\"Finished loading.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to train and test?\n",
        "First, you create and config Hyperparameters: log_path is the direction of saving folder; checkpoint_dir is the direction of your checkpoint folder (optional); dataset is the name of dataset; data_path is is the direction of data folder; split_dir is the direction of folder contains train, validate and test splits. If you want to train or test, create a Trainer object with params is the Hyparparameters that you have already created. Call train() function for running trainer and test() for testing.\n"
      ],
      "metadata": {
        "id": "dFjhdl8ztfMb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBtCNizS6x7S"
      },
      "source": [
        "# Running Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "pb79DVcSOhLx"
      },
      "outputs": [],
      "source": [
        "# HP = Hyperparameters(batch_size=1, num_epochs=5, log_frequency=50, scheduler_step_size=15, img_type=\".jpg\",\n",
        "#                      log_path=\"\",\n",
        "#                      checkpoint_dir=\"\",\n",
        "#                      dataset=\"nyuv2\",\n",
        "#                      data_path=\"/content/drive/MyDrive/nyuv2\",\n",
        "#                      split_dir=\"/content/drive/MyDrive/nyuv2/splits\",)\n",
        "# trainer = Trainer(HP)\n",
        "# trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing\n"
      ],
      "metadata": {
        "id": "U9tR8kw4yU__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# HP = Hyperparameters(batch_size=1, num_epochs=5, log_frequency=50, scheduler_step_size=15, img_type=\".jpg\",\n",
        "#                      log_path=\"/content/drive/MyDrive/nyu_result\",\n",
        "#                      checkpoint_dir=\"/content/drive/MyDrive/Checkpoint_Final/NYUv2\",\n",
        "#                      dataset=\"nyuv2\",\n",
        "#                      data_path=\"/content/drive/MyDrive/nyuv2\",\n",
        "#                      split_dir=\"/content/drive/MyDrive/nyuv2/splits\",)\n",
        "# trainer = Trainer(HP)\n",
        "# trainer.testing()"
      ],
      "metadata": {
        "id": "tqygVyUzyW0f"
      },
      "execution_count": 52,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}